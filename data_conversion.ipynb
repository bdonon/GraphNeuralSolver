{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select path to raw dataset that needs to be converted\n",
    "data_dir = 'datasets/poisson/polygon'\n",
    "\n",
    "A = np.load(os.path.join(data_dir, 'A.npy'), allow_pickle=True)\n",
    "B = np.load(os.path.join(data_dir, 'B.npy'), allow_pickle=True)\n",
    "X = np.load(os.path.join(data_dir, 'X.npy'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert from sparse to dense numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:15<00:00, 662.53it/s]\n",
      "100%|██████████| 10000/10000 [00:15<00:00, 656.87it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = {\n",
    "    'A' : None,\n",
    "    'B' : None,\n",
    "    'X' : None\n",
    "}\n",
    "\n",
    "# Convert A\n",
    "max_edges = 0\n",
    "for i in tqdm.tqdm(range(A.shape[0])):\n",
    "    a_val = A[i].data\n",
    "    a_row = A[i].tocoo().row\n",
    "    a_col = A[i].tocoo().col\n",
    "    a_sample = np.c_[a_row, a_col, a_val]\n",
    "    \n",
    "    # Get maximum amount of edges in dataset:\n",
    "    \n",
    "    max_edges = max(max_edges, np.shape(a_sample)[0])\n",
    "    \n",
    "    \n",
    "for i in tqdm.tqdm(range(A.shape[0])):\n",
    "    a_val = A[i].data\n",
    "    a_row = A[i].tocoo().row\n",
    "    a_col = A[i].tocoo().col\n",
    "    a_sample = np.c_[a_row, a_col, a_val]\n",
    "    \n",
    "    n_edges = np.shape(a_sample)[0]\n",
    "    n_edges_missing = max_edges - n_edges\n",
    "    d_in_A = np.shape(a_sample)[1]\n",
    "    \n",
    "    if n_edges_missing > 0:\n",
    "        a_sample = np.r_[a_sample, np.zeros([n_edges_missing, d_in_A])]\n",
    "    \n",
    "    if dataset['A'] is None:\n",
    "        dataset['A'] = [np.expand_dims(a_sample, 0)]\n",
    "    else:\n",
    "        dataset['A'].append(np.expand_dims(a_sample, 0))\n",
    "dataset['A'] = np.vstack(dataset['A'])\n",
    "\n",
    "# Convert B\n",
    "dataset['B'] = np.expand_dims(B, -1)\n",
    "\n",
    "# Convert X\n",
    "dataset['X'] = np.expand_dims(X, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3044, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['A'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train Val and Test datasets:\n",
    "n_samples = np.shape(dataset['A'])[0]\n",
    "\n",
    "# Select proportions\n",
    "# [%train, %val, %test]\n",
    "proportions = {\n",
    "    'train': 60, \n",
    "    'val' : 20, \n",
    "    'test' : 20\n",
    "}\n",
    "proportions['sum'] = int(proportions['train']+proportions['val']+proportions['test'])\n",
    "    \n",
    "n_train = proportions['train']*n_samples // proportions['sum']\n",
    "n_val = proportions['val']*n_samples // proportions['sum']\n",
    "n_test = n_samples - n_val - n_train\n",
    "\n",
    "# Split\n",
    "dataset_split = {}\n",
    "for key in dataset:\n",
    "    dataset_split[key+'_train'] = dataset[key][:n_train]\n",
    "    dataset_split[key+'_val'] = dataset[key][n_train:n_train+n_val]\n",
    "    dataset_split[key+'_test'] = dataset[key][n_train+n_val:n_train+n_val+n_test]\n",
    "    \n",
    "# Save\n",
    "for key in dataset_split:\n",
    "    np.save(os.path.join(data_dir, key+'.npy'), dataset_split[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "__author__ = \"Sangwoong Yoon\"\n",
    "\n",
    "def np_to_tfrecords(A, B, X, file_path_prefix, verbose=True):\n",
    "    \"\"\"\n",
    "    author : \"Sangwoong Yoon\"\n",
    "    \"\"\"\n",
    "    def _dtype_feature(ndarray):\n",
    "        \"\"\"match appropriate tf.train.Feature class with dtype of ndarray. \"\"\"\n",
    "        assert isinstance(ndarray, np.ndarray)\n",
    "        dtype_ = ndarray.dtype\n",
    "        if dtype_ == np.float64 or dtype_ == np.float32:\n",
    "            return lambda array: tf.train.Feature(float_list=tf.train.FloatList(value=array))\n",
    "        elif dtype_ == np.int64:\n",
    "            return lambda array: tf.train.Feature(int64_list=tf.train.Int64List(value=array))\n",
    "        else:  \n",
    "            raise ValueError(\"The input should be numpy ndarray. \\\n",
    "                               Instaed got {}\".format(ndarray.dtype))\n",
    "            \n",
    "    assert isinstance(A, np.ndarray)\n",
    "    assert len(A.shape) == 2\n",
    "    \n",
    "    assert isinstance(B, np.ndarray)\n",
    "    assert len(B.shape) == 2\n",
    "    \n",
    "    assert isinstance(X, np.ndarray)\n",
    "    assert len(X.shape) == 2\n",
    "    \n",
    "    # load appropriate tf.train.Feature class depending on dtype\n",
    "    dtype_feature_a = _dtype_feature(A)\n",
    "    dtype_feature_b = _dtype_feature(B)\n",
    "    dtype_feature_x = _dtype_feature(X)      \n",
    "        \n",
    "    # Generate tfrecord writer\n",
    "    result_tf_file = file_path_prefix + '.tfrecords'\n",
    "    writer = tf.python_io.TFRecordWriter(result_tf_file)\n",
    "    if verbose:\n",
    "        print(\"Serializing {:d} examples into {}\".format(X.shape[0], result_tf_file))\n",
    "        \n",
    "    # iterate over each sample,\n",
    "    # and serialize it as ProtoBuf.\n",
    "    for idx in tqdm.tqdm(range(A.shape[0])):\n",
    "        a = A[idx]\n",
    "        b = B[idx]\n",
    "        x = X[idx]\n",
    "        \n",
    "        d_feature = {}\n",
    "        d_feature['A'] = dtype_feature_a(a)\n",
    "        d_feature['B'] = dtype_feature_b(b)\n",
    "        d_feature['X'] = dtype_feature_x(x)\n",
    "        \n",
    "            \n",
    "        features = tf.train.Features(feature=d_feature)\n",
    "        example = tf.train.Example(features=features)\n",
    "        serialized = example.SerializeToString()\n",
    "        writer.write(serialized)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Writing {} done!\".format(result_tf_file))\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/6000 [00:00<01:18, 76.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serializing 6000 examples into datasets/poisson/polygon/train.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [01:05<00:00, 91.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing datasets/poisson/polygon/train.tfrecords done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/2000 [00:00<00:28, 69.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serializing 2000 examples into datasets/poisson/polygon/val.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:19<00:00, 102.10it/s]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing datasets/poisson/polygon/val.tfrecords done!\n",
      "Serializing 2000 examples into datasets/poisson/polygon/test.tfrecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:17<00:00, 115.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing datasets/poisson/polygon/test.tfrecords done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for mode in ['train', 'val', 'test']:\n",
    "\n",
    "    A = np.load(os.path.join(data_dir, 'A_'+mode+'.npy'), allow_pickle=True)\n",
    "    B = np.load(os.path.join(data_dir, 'B_'+mode+'.npy'), allow_pickle=True)\n",
    "    X = np.load(os.path.join(data_dir, 'X_'+mode+'.npy'), allow_pickle=True)\n",
    "\n",
    "    n_samples = np.array(np.shape(A)[0])\n",
    "    n_edges = np.array(np.shape(A)[1])\n",
    "    d_in_A = np.array(np.shape(A)[2])\n",
    "    \n",
    "    n_nodes = np.array(np.shape(B)[1])\n",
    "    d_in_B = np.array(np.shape(B)[2])\n",
    "\n",
    "    A = np.reshape(A, [n_samples, -1])\n",
    "    B = np.reshape(B, [n_samples, -1])\n",
    "    X = np.reshape(X, [n_samples, -1])\n",
    "\n",
    "    np_to_tfrecords(A, B, X, os.path.join(data_dir, mode), \n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
